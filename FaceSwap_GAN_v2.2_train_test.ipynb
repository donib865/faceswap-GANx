{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umeyama import umeyama\n",
    "from image_augmentation import random_transform\n",
    "from prefetch_generator import background # !pip install prefetch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from pathlib import PurePath, Path\n",
    "from random import randint, shuffle\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_learning_phase(1)\n",
    "# K.set_learning_phase(0) # set to 0 in inference phase (video conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input/Output resolution\n",
    "RESOLUTION = 64 # 64x64, 128x128, 256x256\n",
    "assert (RESOLUTION % 64) == 0, \"RESOLUTION should be 64, 128, or 256.\"\n",
    "\n",
    "# Batch size\n",
    "batchSize = 8\n",
    "\n",
    "# Use motion blurs (data augmentation)\n",
    "# set True if training data contains images extracted from videos\n",
    "use_da_motion_blur = False \n",
    "\n",
    "# Probability of random color matching (data augmentation)\n",
    "prob_random_color_match = 0.5\n",
    "\n",
    "# Path to training images\n",
    "img_dirA = './faceA'\n",
    "img_dirB = './faceB'\n",
    "img_dirA_bm_eyes = \"./binary_masks/faceA_eyes\"\n",
    "img_dirB_bm_eyes = \"./binary_masks/faceB_eyes\"\n",
    "\n",
    "# Path to saved model weights\n",
    "models_dir = \"./models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture configuration\n",
    "arch_config = {}\n",
    "arch_config['IMAGE_SHAPE'] = (RESOLUTION, RESOLUTION, 3)\n",
    "arch_config['use_self_attn'] = True\n",
    "arch_config['norm'] = \"instancenorm\" # instancenorm, batchnorm, layernorm, groupnorm, none\n",
    "arch_config['model_capacity'] = \"standard\" # standard, lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function weights configuration\n",
    "loss_weights = {}\n",
    "loss_weights['w_D'] = 0.1 # Discriminator\n",
    "loss_weights['w_recon'] = 1. # L1 reconstruction loss\n",
    "loss_weights['w_edge'] = 0.1 # edge loss\n",
    "loss_weights['w_eyes'] = 30. # reconstruction and edge loss on eyes area\n",
    "loss_weights['w_pl'] = (0.01, 0.1, 0.3, 0.1) # perceptual loss (0.003, 0.03, 0.3, 0.3)\n",
    "\n",
    "# Init. loss config.\n",
    "loss_config = {}\n",
    "loss_config[\"gan_training\"] = \"mixup_LSGAN\" # \"mixup_LSGAN\" or \"relativistic_avg_LSGAN\"\n",
    "loss_config['use_PL'] = False\n",
    "loss_config['use_mask_hinge_loss'] = False\n",
    "loss_config['m_mask'] = 0.\n",
    "loss_config['lr_factor'] = 1.\n",
    "loss_config['use_cyclic_loss'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "# Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.faceswap_gan_model import FaceswapGANModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = FaceswapGANModel(**arch_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "# Load Model Weights\n",
    "\n",
    "Weights file names:\n",
    "```python\n",
    "encoder.h5, decoder_A.h5, deocder_B.h5, netDA.h5, netDB.h5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights files are successfully loaded\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(path=models_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (The following cells are for training, jump to [transform_face()](#tf) or [video conversion](#vc) for inference.)\n",
    "\n",
    "# Define Losses and Build Training Functions\n",
    "\n",
    "TODO: split into two methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/rcmalli/keras-vggface\n",
    "#!pip install keras_vggface\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "# VGGFace ResNet50\n",
    "vggface = VGGFace(include_top=False, model='resnet50', input_shape=(224, 224, 3))\n",
    "\n",
    "#vggface.summary()\n",
    "\n",
    "model.build_pl_model(vggface_model=vggface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_train_functions(loss_weights=loss_weights, **loss_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='9'></a>\n",
    "# DataLoader\n",
    "\n",
    "TODO: write a DataLoader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motion blurs as data augmentation\n",
    "def get_motion_blur_kernel(sz=7):\n",
    "    rot_angle = np.random.uniform(-180,180)\n",
    "    kernel = np.zeros((sz,sz))\n",
    "    kernel[int((sz-1)//2), :] = np.ones(sz)\n",
    "    kernel = ndimage.interpolation.rotate(kernel, rot_angle, reshape=False)\n",
    "    kernel = np.clip(kernel, 0, 1)\n",
    "    normalize_factor = 1 / np.sum(kernel)\n",
    "    kernel = kernel * normalize_factor\n",
    "    return kernel\n",
    "\n",
    "def motion_blur(images, sz=7):\n",
    "    # images is a list [image2, image2, ...]\n",
    "    blur_sz = np.random.choice([5, 7, 9, 11])\n",
    "    kernel_motion_blur = get_motion_blur_kernel(blur_sz)\n",
    "    for i, image in enumerate(images):\n",
    "        images[i] = cv2.filter2D(image, -1, kernel_motion_blur).astype(np.float64)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils for loading data\n",
    "def load_data(file_pattern):\n",
    "    return glob.glob(file_pattern)\n",
    "  \n",
    "def random_warp_rev(image, res=RESOLUTION):\n",
    "    assert image.shape == (256,256,6)\n",
    "    res_scale = res//64\n",
    "    assert res_scale >= 1, f\"Resolution should be >= 64. Recieved {res}.\"\n",
    "    interp_param = 80 * res_scale\n",
    "    interp_slice = slice(interp_param//10,9*interp_param//10)\n",
    "    dst_pnts_slice = slice(0,65*res_scale,16*res_scale)\n",
    "    \n",
    "    rand_coverage = np.random.randint(20) + 80 # random warping coverage\n",
    "    rand_scale = np.random.uniform(5., 6.2) # random warping scale\n",
    "    \n",
    "    range_ = np.linspace(128-rand_coverage, 128+rand_coverage, 5)\n",
    "    mapx = np.broadcast_to(range_, (5,5))\n",
    "    mapy = mapx.T\n",
    "    mapx = mapx + np.random.normal(size=(5,5), scale=rand_scale)\n",
    "    mapy = mapy + np.random.normal(size=(5,5), scale=rand_scale)\n",
    "    interp_mapx = cv2.resize(mapx, (interp_param,interp_param))[interp_slice,interp_slice].astype('float32')\n",
    "    interp_mapy = cv2.resize(mapy, (interp_param,interp_param))[interp_slice,interp_slice].astype('float32')\n",
    "    warped_image = cv2.remap(image, interp_mapx, interp_mapy, cv2.INTER_LINEAR)\n",
    "    src_points = np.stack([mapx.ravel(), mapy.ravel()], axis=-1)\n",
    "    dst_points = np.mgrid[dst_pnts_slice,dst_pnts_slice].T.reshape(-1,2)\n",
    "    mat = umeyama(src_points, dst_points, True)[0:2]\n",
    "    target_image = cv2.warpAffine(image, mat, (res,res))\n",
    "    return warped_image, target_image\n",
    "\n",
    "def random_color_match(image):\n",
    "    global fns_all_trn_data\n",
    "    rand_idx = np.random.randint(len(fns_all_trn_data))\n",
    "    fn_match = fns_all_trn_data[rand_idx]\n",
    "    tar_img = cv2.imread(fn_match)\n",
    "    if tar_img is None:\n",
    "        print(f\"Failed reading image {fn_match} in random_color_match().\")\n",
    "        return image\n",
    "    r = 60\n",
    "    src_img = cv2.resize(image, (256,256))\n",
    "    tar_img = cv2.resize(tar_img, (256,256))\n",
    "    mt = np.mean(tar_img[r:-r,r:-r,:], axis=(0,1))\n",
    "    st = np.std(tar_img[r:-r,r:-r,:], axis=(0,1))\n",
    "    ms = np.mean(src_img[r:-r,r:-r,:], axis=(0,1))\n",
    "    ss = np.std(src_img[r:-r,r:-r,:], axis=(0,1))    \n",
    "    if ss.any() <= 1e-7: return src_img    \n",
    "    result = st * (src_img.astype(np.float32) - ms) / (ss+1e-7) + mt\n",
    "    result = result - result.min()\n",
    "    result = (255.0/result.max()*result).astype(np.float32)\n",
    "    return result\n",
    "\n",
    "random_transform_args = {\n",
    "    'rotation_range': 10,\n",
    "    'zoom_range': 0.1,\n",
    "    'shift_range': 0.05,\n",
    "    'random_flip': 0.5,\n",
    "    }\n",
    "def read_image(fn, dir_bm_eyes=None, random_transform_args=random_transform_args):\n",
    "    assert dir_bm_eyes is not None\n",
    "    raw_fn = PurePath(fn).parts[-1]\n",
    "    image = cv2.imread(fn)\n",
    "    assert image is not None, f\"Failed reading image {fn}.\"\n",
    "    if np.random.uniform() >= prob_random_color_match:\n",
    "        image = random_color_match(image)\n",
    "    image = cv2.resize(image, (256,256)) / 255 * 2 - 1\n",
    "    bm_eyes = cv2.imread(f\"{dir_bm_eyes}/{raw_fn}\")\n",
    "    assert bm_eyes is not None, f\"Failed reading binary mask {dir_bm_eyes}/{raw_fn}.\"\n",
    "    bm_eyes = cv2.resize(bm_eyes, (256,256)) / 255.\n",
    "    image = np.concatenate([image, bm_eyes], axis=-1)\n",
    "    image = random_transform(image, **random_transform_args)\n",
    "    warped_img, target_img = random_warp_rev(image)\n",
    "    \n",
    "    bm_eyes = target_img[...,3:]\n",
    "    warped_img = warped_img[...,:3]\n",
    "    target_img = target_img[...,:3]\n",
    "    \n",
    "    # Motion blur data augmentation:\n",
    "    # we want the model to learn to preserve motion blurs of input images\n",
    "    if np.random.uniform() < 0.25 and use_da_motion_blur: \n",
    "        warped_img, target_img = motion_blur([warped_img, target_img])\n",
    "    \n",
    "    return warped_img, target_img, bm_eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A generator function that yields epoch and data\n",
    "@background(32)\n",
    "def minibatch(data, batchsize, dir_bm_eyes):\n",
    "    length = len(data)\n",
    "    epoch = i = 0\n",
    "    tmpsize = None  \n",
    "    shuffle(data)\n",
    "    while True:\n",
    "        size = tmpsize if tmpsize else batchsize\n",
    "        if i+size > length:\n",
    "            shuffle(data)\n",
    "            i = 0\n",
    "            epoch+=1        \n",
    "        rtn = np.float32([read_image(data[j], dir_bm_eyes) for j in range(i,i+size)])\n",
    "        i+=size\n",
    "        tmpsize = yield epoch, rtn[:,0,:,:,:], rtn[:,1,:,:,:], rtn[:,2,:,:,:]       \n",
    "\n",
    "def create_minibatch(data, batchsize, dir_bm_eyes):\n",
    "    # This is a redundant function, to be written in to a DataLoader class.\n",
    "    batch = minibatch(data, batchsize, dir_bm_eyes)\n",
    "    tmpsize = None    \n",
    "    while True:        \n",
    "        ep1, warped_img, target_img, bm_eyes = next(batch)\n",
    "        tmpsize = yield ep1, warped_img, target_img, bm_eyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizer\n",
    "\n",
    "TODO: write a Visualizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import showG, showG_mask, showG_eyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "# Start Training\n",
    "TODO: make training script compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ./models directory\n",
    "Path(f\"models\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in folder A: 376\n",
      "Number of images in folder B: 318\n"
     ]
    }
   ],
   "source": [
    "# Get filenames\n",
    "train_A = load_data(img_dirA+\"/*.*\")\n",
    "train_B = load_data(img_dirB+\"/*.*\")\n",
    "\n",
    "global fns_all_trn_data\n",
    "fns_all_trn_data = train_A + train_B\n",
    "\n",
    "assert len(train_A), \"No image found in \" + str(img_dirA)\n",
    "assert len(train_B), \"No image found in \" + str(img_dirB)\n",
    "print (\"Number of images in folder A: \" + str(len(train_A)))\n",
    "print (\"Number of images in folder B: \" + str(len(train_B)))\n",
    "\n",
    "assert len(glob.glob(img_dirA_bm_eyes+\"/*.*\")), \"No binary mask found in \" + str(img_dirA_bm_eyes)\n",
    "assert len(glob.glob(img_dirB_bm_eyes+\"/*.*\")), \"No binary mask found in \" + str(img_dirB_bm_eyes)\n",
    "assert len(glob.glob(img_dirA_bm_eyes+\"/*.*\")) == len(train_A), \\\n",
    "\"Number of faceA images does not match number of their binary masks. Can be caused by any none image file in the folder.\"\n",
    "assert len(glob.glob(img_dirB_bm_eyes+\"/*.*\")) == len(train_B), \\\n",
    "\"Number of faceB images does not match number of their binary masks. Can be caused by any none image file in the folder.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_loss_config(loss_config):\n",
    "    for config, value in loss_config.items():\n",
    "        print(f\"{config} = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random binary masks of eyes\n",
    "train_batchA = create_minibatch(train_A, batchSize, img_dirA_bm_eyes)\n",
    "train_batchB = create_minibatch(train_B, batchSize, img_dirB_bm_eyes)\n",
    "_, _, tA, bmA = next(train_batchA)  \n",
    "_, _, tB, bmB = next(train_batchB)\n",
    "showG_eyes(tA, tB, bmA, bmB, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_session(save_path):\n",
    "    global model, vggface\n",
    "    model.save_weights(path=save_path)\n",
    "    del model\n",
    "    del vggface\n",
    "    K.clear_session()\n",
    "    model = FaceswapGANModel(**arch_config)\n",
    "    model.load_weights(path=save_path)\n",
    "    vggface = VGGFace(include_top=False, model='resnet50', input_shape=(224, 224, 3))\n",
    "    model.build_pl_model(vggface_model=vggface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "t0 = time.time()\n",
    "gen_iterations = 0\n",
    "errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
    "errGAs = {}\n",
    "errGBs = {}\n",
    "# Dictionaries are ordered in Python 3.6\n",
    "for k in ['ttl', 'adv', 'recon', 'edge', 'pl']:\n",
    "    errGAs[k] = 0\n",
    "    errGBs[k] = 0\n",
    "\n",
    "display_iters = 300\n",
    "backup_iters = 5000\n",
    "TOTAL_ITERS = 40000\n",
    "\n",
    "train_batchA = create_minibatch(train_A, batchSize, img_dirA_bm_eyes)\n",
    "train_batchB = create_minibatch(train_B, batchSize, img_dirB_bm_eyes)\n",
    "\n",
    "while gen_iterations <= TOTAL_ITERS:  \n",
    "    data_A = next(train_batchA) \n",
    "    data_B = next(train_batchB) \n",
    "    \n",
    "    # Loss function automation\n",
    "    if gen_iterations == (TOTAL_ITERS//5 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = False\n",
    "        loss_config['m_mask'] = 0.0\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    elif gen_iterations == (TOTAL_ITERS//5 + TOTAL_ITERS//10 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = True\n",
    "        loss_config['m_mask'] = 0.5\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Complete.\")\n",
    "    elif gen_iterations == (2*TOTAL_ITERS//5 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = True\n",
    "        loss_config['m_mask'] = 0.2\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    elif gen_iterations == (TOTAL_ITERS//2 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = True\n",
    "        loss_config['m_mask'] = 0.4\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    elif gen_iterations == (2*TOTAL_ITERS//3 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = False\n",
    "        loss_config['m_mask'] = 0.\n",
    "        loss_config['lr_factor'] = 0.3\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    elif gen_iterations == (8*TOTAL_ITERS//10 - display_iters//2):\n",
    "        clear_output()\n",
    "        model.decoder_A.load_weights(\"models/decoder_B.h5\") # swap decoders\n",
    "        model.decoder_B.load_weights(\"models/decoder_A.h5\") # swap decoders\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = True\n",
    "        loss_config['m_mask'] = 0.1\n",
    "        loss_config['lr_factor'] = 0.3\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    elif gen_iterations == (9*TOTAL_ITERS//10 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = False\n",
    "        loss_config['m_mask'] = 0.0\n",
    "        loss_config['lr_factor'] = 0.1\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    if gen_iterations == 5:\n",
    "        print (\"working.\")\n",
    "    \n",
    "    # Train dicriminators for one batch\n",
    "    errDA, errDB = model.train_one_batch_D(data_A=data_A, data_B=data_B)\n",
    "    errDA_sum +=errDA[0]\n",
    "    errDB_sum +=errDB[0]\n",
    "\n",
    "    # Train generators for one batch\n",
    "    errGA, errGB = model.train_one_batch_G(data_A=data_A, data_B=data_B)\n",
    "    errGA_sum += errGA[0]\n",
    "    errGB_sum += errGB[0]\n",
    "    for i, k in enumerate(['ttl', 'adv', 'recon', 'edge', 'pl']):\n",
    "        errGAs[k] += errGA[i]\n",
    "        errGBs[k] += errGB[i]\n",
    "    gen_iterations+=1\n",
    "    \n",
    "    # Visualization\n",
    "    if gen_iterations % display_iters == 0:\n",
    "        clear_output()\n",
    "            \n",
    "        # Display loss information\n",
    "        show_loss_config(loss_config)\n",
    "        print(\"----------\") \n",
    "        print('[iter %d] Loss_DA: %f Loss_DB: %f Loss_GA: %f Loss_GB: %f time: %f'\n",
    "        % (gen_iterations, errDA_sum/display_iters, errDB_sum/display_iters,\n",
    "           errGA_sum/display_iters, errGB_sum/display_iters, time.time()-t0))  \n",
    "        print(\"----------\") \n",
    "        print(\"Generator loss details:\")\n",
    "        print(f'[Adversarial loss]')  \n",
    "        print(f'GA: {errGAs[\"adv\"]/display_iters:.4f} GB: {errGBs[\"adv\"]/display_iters:.4f}')\n",
    "        print(f'[Reconstruction loss]')\n",
    "        print(f'GA: {errGAs[\"recon\"]/display_iters:.4f} GB: {errGBs[\"recon\"]/display_iters:.4f}')\n",
    "        print(f'[Edge loss]')\n",
    "        print(f'GA: {errGAs[\"edge\"]/display_iters:.4f} GB: {errGBs[\"edge\"]/display_iters:.4f}')\n",
    "        if loss_config['use_PL'] == True:\n",
    "            print(f'[Perceptual loss]')\n",
    "            try:\n",
    "                print(f'GA: {errGAs[\"pl\"][0]/display_iters:.4f} GB: {errGBs[\"pl\"][0]/display_iters:.4f}')\n",
    "            except:\n",
    "                print(f'GA: {errGAs[\"pl\"]/display_iters:.4f} GB: {errGBs[\"pl\"]/display_iters:.4f}')\n",
    "        \n",
    "        # Display images\n",
    "        print(\"----------\") \n",
    "        _, wA, tA, _ = next(train_batchA)  \n",
    "        _, wB, tB, _ = next(train_batchB)\n",
    "        print(\"Transformed (masked) results:\")\n",
    "        showG(tA, tB, model.path_A, model.path_B, batchSize)   \n",
    "        print(\"Masks:\")\n",
    "        showG_mask(tA, tB, model.path_mask_A, model.path_mask_B, batchSize)  \n",
    "        print(\"Reconstruction results:\")\n",
    "        showG(wA, wB, model.path_bgr_A, model.path_bgr_B, batchSize)           \n",
    "        errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
    "        for k in ['ttl', 'adv', 'recon', 'edge', 'pl']:\n",
    "            errGAs[k] = 0\n",
    "            errGBs[k] = 0\n",
    "        \n",
    "        # Save models\n",
    "        model.save_weights(path=models_dir)\n",
    "    \n",
    "    # Backup models\n",
    "    if gen_iterations % backup_iters == 0: \n",
    "        bkup_dir = f\"{models_dir}/backup_iter{gen_iterations}\"\n",
    "        Path(bkup_dir).mkdir(parents=True, exist_ok=True)\n",
    "        model.save_weights(path=bkup_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further train the model adding cycle consistency loss (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_config = {}\n",
    "#loss_config['use_PL'] = True\n",
    "#loss_config['use_mask_hinge_loss'] = False\n",
    "#loss_config['m_mask'] = 0.\n",
    "#loss_config['lr_factor'] = 0.1\n",
    "#loss_config['use_cyclic_loss'] = True\n",
    "#model.build_train_functions(loss_weights=loss_weights, **loss_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t0 = time.time()\n",
    "#gen_iterations = 0\n",
    "#epoch = 0\n",
    "#errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
    "#\n",
    "#display_iters = 300\n",
    "#batchSize = batchSize//2\n",
    "#train_batchA = create_minibatch(train_A, batchSize)\n",
    "#train_batchB = create_minibatch(train_B, batchSize)\n",
    "#\n",
    "## ========== Change TOTAL_ITERS to desired iterations  ========== \n",
    "#TOTAL_ITERS = 10000\n",
    "#\n",
    "#while gen_iterations <= TOTAL_ITERS:  \n",
    "#    data_A = next(train_batchA) \n",
    "#    data_B = next(train_batchB)\n",
    "#    \n",
    "#    # Train dicriminators for one batch\n",
    "#    if gen_iterations % 1 == 0:\n",
    "#        errDA, errDB = model.train_one_batch_D(data_A=data_A, data_B=data_B)\n",
    "#    errDA_sum +=errDA[0]\n",
    "#    errDB_sum +=errDB[0]\n",
    "#    \n",
    "#    if gen_iterations == 5:\n",
    "#        print (\"working.\")\n",
    "#\n",
    "#    # Train generators for one batch\n",
    "#    errGA, errGB = model.train_one_batch_G(data_A=data_A, data_B=data_B)\n",
    "#    errGA_sum += errGA[0]\n",
    "#    errGB_sum += errGB[0]\n",
    "#    gen_iterations+=1\n",
    "#    \n",
    "#    if gen_iterations % display_iters == 0:\n",
    "#        if gen_iterations % (display_iters) == 0:\n",
    "#            clear_output()\n",
    "#        show_loss_config(loss_config)\n",
    "#        print('[iter %d] Loss_DA: %f Loss_DB: %f Loss_GA: %f Loss_GB: %f time: %f'\n",
    "#        % (gen_iterations, errDA_sum/display_iters, errDB_sum/display_iters,\n",
    "#           errGA_sum/display_iters, errGB_sum/display_iters, time.time()-t0))   \n",
    "#        \n",
    "#        # get new batch of images and generate results for visualization\n",
    "#        for _ in range(1):           \n",
    "#            print(\"----------\") \n",
    "#            _, wA, tA, _ = next(train_batchA)  \n",
    "#            _, wB, tB, _ = next(train_batchB)\n",
    "#            showG(tA, tB, model.path_A, model.path_B)   \n",
    "#            print(\"\")\n",
    "#            showG(wA, wB, model.path_bgr_A, model.path_bgr_B)  \n",
    "#            print(\"\")\n",
    "#            showG_mask(tA, tB, model.path_mask_A, model.path_mask_B)           \n",
    "#        errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
    "#        \n",
    "#        # Save models (save with latest weights)\n",
    "#        model.encoder.save_weights(\"models/encoder.h5\")\n",
    "#        model.decoder_A.save_weights(\"models/decoder_A.h5\")\n",
    "#        model.decoder_B.save_weights(\"models/decoder_B.h5\")\n",
    "#        model.netDA.save_weights(\"models/netDA.h5\")\n",
    "#        model.netDB.save_weights(\"models/netDB.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tf'></a>\n",
    "# Helper Function for Single Image Transformation: transform_face()\n",
    "\n",
    "    INPUTS:\n",
    "        inp_img: A RGB face image of any size.\n",
    "        path_func: a function that is either path_abgr_A or path_abgr_B.\n",
    "        roi_coef: A coefficient that affects the cropped center area\n",
    "        color_correction: boolean, whether use color correction or not\n",
    "    OUPUTS:\n",
    "        result_img: A RGB swapped face image after masking.\n",
    "        result_mask: The alpha mask image which correspons to the result_img."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_n_scale(src_img, tar_img):\n",
    "    mt = np.mean(tar_img, axis=(0,1))\n",
    "    st = np.std(tar_img, axis=(0,1))\n",
    "    ms = np.mean(src_img, axis=(0,1))\n",
    "    ss = np.std(src_img, axis=(0,1))    \n",
    "    if ss.any() <= 1e-7: return src_img    \n",
    "    result = st * (src_img.astype(np.float32) - ms) / (ss+1e-7) + mt\n",
    "    result = (255.0/result.max()*result).astype(np.float32)\n",
    "    return result\n",
    "\n",
    "def transform_face(inp_img, direction=\"AtoB\", roi_coef=15, color_correction=False):\n",
    "    def get_feather_edges_mask(img, roi_coef):\n",
    "        img_size = img.shape\n",
    "        mask = np.zeros_like(img)\n",
    "        mask[img_size[0]//roi_coef:-img_size[0]//roi_coef, \n",
    "             img_size[1]//roi_coef:-img_size[1]//roi_coef,:]  = 255\n",
    "        mask = cv2.GaussianBlur(mask,(15,15),10)\n",
    "        return mask        \n",
    "\n",
    "    if direction == \"AtoB\":\n",
    "        path_func = model.path_abgr_B\n",
    "    elif direction == \"BtoA\":\n",
    "        path_func = model.path_abgr_A\n",
    "    else:\n",
    "        raise ValueError(f\"direction should be either AtoB or BtoA, recieved {direction}.\")\n",
    "\n",
    "    # pre-process input image\n",
    "    img_bgr = cv2.cvtColor(inp_img, cv2.COLOR_RGB2BGR)\n",
    "    input_size = img_bgr.shape        \n",
    "    roi = img_bgr[input_size[0]//roi_coef:-input_size[0]//roi_coef, \n",
    "                  input_size[1]//roi_coef:-input_size[1]//roi_coef,:] # BGR, [0, 255]  \n",
    "    roi_size = roi.shape\n",
    "    ae_input = cv2.resize(roi, (RESOLUTION,RESOLUTION))/255. * 2 - 1 # BGR, [-1, 1]       \n",
    "\n",
    "    # post-process transformed roi image\n",
    "    ae_output = np.squeeze(np.array([path_func([[ae_input]])]))\n",
    "    ae_output_a = ae_output[:,:,0] * 255\n",
    "    ae_output_a = cv2.resize(ae_output_a, (roi_size[1],roi_size[0]))[...,np.newaxis]\n",
    "    ae_output_bgr = np.clip( (ae_output[:,:,1:] + 1) * 255 / 2, 0, 255)\n",
    "    ae_output_bgr = cv2.resize(ae_output_bgr, (roi_size[1],roi_size[0]))\n",
    "    ae_output_masked = (ae_output_a/255 * ae_output_bgr + (1 - ae_output_a/255) * roi).astype('uint8') # BGR, [0, 255]\n",
    "    if color_correction:\n",
    "        ae_output_masked = shift_n_scale(ae_output_masked, roi)\n",
    "\n",
    "    # merge transformed output back to input image\n",
    "    blend_mask = get_feather_edges_mask(roi, roi_coef)        \n",
    "    blended_img = blend_mask/255 * ae_output_masked + (1-blend_mask/255) * roi\n",
    "    result = img_bgr\n",
    "    result[input_size[0]//roi_coef:-input_size[0]//roi_coef, \\\n",
    "           input_size[1]//roi_coef:-input_size[1]//roi_coef,:] = blended_img \n",
    "    result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB) \n",
    "    result_alpha = np.zeros_like(img_bgr)\n",
    "    result_alpha[input_size[0]//roi_coef:-input_size[0]//roi_coef, \\\n",
    "                 input_size[1]//roi_coef:-input_size[1]//roi_coef,:] = (blend_mask/255) * ae_output_a \n",
    "    return result, result_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = plt.imread(\"./TEST_IMAGE.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_img, result_mask = transform_face(input_img, direction=\"BtoA\", roi_coef=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(result_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(result_mask[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vc'></a>\n",
    "# Video Conversion\n",
    "\n",
    "TODO: write another script for inference and video conversion\n",
    "\n",
    "### Note: Code for video conversion is loosely organized and not well maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ffmpeg if needed, which is required by moviepy.\n",
    "\n",
    "#import imageio\n",
    "#imageio.plugins.ffmpeg.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import mtcnn_detect_face\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build MTCNN for face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mtcnn(sess, model_path):\n",
    "    if not model_path:\n",
    "        model_path,_ = os.path.split(os.path.realpath(__file__))\n",
    "\n",
    "    with tf.variable_scope('pnet2'):\n",
    "        data = tf.placeholder(tf.float32, (None,None,None,3), 'input')\n",
    "        pnet = mtcnn_detect_face.PNet({'data':data})\n",
    "        pnet.load(os.path.join(model_path, 'det1.npy'), sess)\n",
    "    with tf.variable_scope('rnet2'):\n",
    "        data = tf.placeholder(tf.float32, (None,24,24,3), 'input')\n",
    "        rnet = mtcnn_detect_face.RNet({'data':data})\n",
    "        rnet.load(os.path.join(model_path, 'det2.npy'), sess)\n",
    "    with tf.variable_scope('onet2'):\n",
    "        data = tf.placeholder(tf.float32, (None,48,48,3), 'input')\n",
    "        onet = mtcnn_detect_face.ONet({'data':data})\n",
    "        onet.load(os.path.join(model_path, 'det3.npy'), sess)\n",
    "    return pnet, rnet, onet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = \"./mtcnn_weights/\"\n",
    "\n",
    "sess = K.get_session()\n",
    "with sess.as_default():\n",
    "    global pnet, rnet, onet \n",
    "    pnet2, rnet2, onet2 = create_mtcnn(sess, WEIGHTS_PATH)\n",
    "\n",
    "global pnet, rnet, onet\n",
    "pnet_fun = K.function([pnet2.layers['data']],[pnet2.layers['conv4-2'], pnet2.layers['prob1']])\n",
    "rnet_fun = K.function([rnet2.layers['data']],[rnet2.layers['conv5-2'], rnet2.layers['prob1']])\n",
    "onet_fun = K.function([onet2.layers['data']], [onet2.layers['conv6-2'], onet2.layers['conv6-3'], onet2.layers['prob1']])\n",
    "\n",
    "with tf.variable_scope('pnet2', reuse=True):\n",
    "    data = tf.placeholder(tf.float32, (None,None,None,3), 'input')\n",
    "    pnet2 = mtcnn_detect_face.PNet({'data':data})\n",
    "    pnet2.load(os.path.join(\"./mtcnn_weights/\", 'det1.npy'), sess)\n",
    "with tf.variable_scope('rnet2', reuse=True):\n",
    "    data = tf.placeholder(tf.float32, (None,24,24,3), 'input')\n",
    "    rnet2 = mtcnn_detect_face.RNet({'data':data})\n",
    "    rnet2.load(os.path.join(\"./mtcnn_weights/\", 'det2.npy'), sess)\n",
    "with tf.variable_scope('onet2', reuse=True):\n",
    "    data = tf.placeholder(tf.float32, (None,48,48,3), 'input')\n",
    "    onet2 = mtcnn_detect_face.ONet({'data':data})\n",
    "    onet2.load(os.path.join(\"./mtcnn_weights/\", 'det3.npy'), sess)\n",
    "    \n",
    "pnet = K.function([pnet2.layers['data']],[pnet2.layers['conv4-2'], pnet2.layers['prob1']])\n",
    "rnet = K.function([rnet2.layers['data']],[rnet2.layers['conv5-2'], rnet2.layers['prob1']])\n",
    "onet = K.function([onet2.layers['data']], [onet2.layers['conv6-2'], onet2.layers['conv6-3'], onet2.layers['prob1']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils for video conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_smoothed_mask = True\n",
    "use_smoothed_bbox = True\n",
    "\n",
    "def get_src_landmarks(x0, x1, y0, y1, pnts):\n",
    "    \"\"\"\n",
    "    x0, x1, y0, y1: (smoothed) bbox coord.\n",
    "    pnts: landmarks predicted by MTCNN\n",
    "    \"\"\"    \n",
    "    src_landmarks = []\n",
    "    for i in range(5):\n",
    "        src_landmarks.extend([(int(pnts[i+5][0]-x0), int(pnts[i][0]-y0))])\n",
    "    return src_landmarks\n",
    "\n",
    "def get_tar_landmarks(img):\n",
    "    \"\"\"    \n",
    "    img: detected face image\n",
    "    \"\"\"         \n",
    "    avg_landmarks = [\n",
    "        (0.31339227236234224, 0.3259269274198092),\n",
    "        (0.31075140146108776, 0.7228453709528997),\n",
    "        (0.5523683107816256, 0.5187296867370605),\n",
    "        (0.7752419985257663, 0.37262483743520886),\n",
    "        (0.7759613623985877, 0.6772957581740159)\n",
    "        ]   \n",
    "    \n",
    "    tar_landmarks = []\n",
    "    img_size = img.shape\n",
    "    for xy in avg_landmarks: \n",
    "        tar_landmarks.extend([(int(xy[0]*img_size[0]), int(xy[1]*img_size[1]))])\n",
    "    return tar_landmarks\n",
    "\n",
    "def landmarks_match_mtcnn(src_im, src_landmarks, tar_landmarks):  \n",
    "    # umeyama(src, dst, estimate_scale)\n",
    "    # landmarks coord. should be (width, height) or (y, x)\n",
    "    src_size = src_im.shape\n",
    "    src_tmp = []\n",
    "    dst_tmp = []\n",
    "    for xy in src_landmarks:\n",
    "        src_tmp.extend([(int(xy[1]), int(xy[0]))])\n",
    "    for xy in tar_landmarks:\n",
    "        dst_tmp.extend([(int(xy[1]), int(xy[0]))])\n",
    "    M = umeyama(np.array(src_tmp), np.array(dst_tmp), True)[0:2]\n",
    "    result = cv2.warpAffine(src_im, M, (src_size[1], src_size[0]), borderMode=cv2.BORDER_REPLICATE) \n",
    "    return result\n",
    "\n",
    "def shift_n_scale(src_img, tar_img):\n",
    "    # https://github.com/ftokarev/tf-adain/blob/master/adain/norm.py\n",
    "    mt = np.mean(tar_img, axis=(0,1))\n",
    "    st = np.std(tar_img, axis=(0,1))\n",
    "    ms = np.mean(src_img, axis=(0,1))\n",
    "    ss = np.std(src_img, axis=(0,1))    \n",
    "    if ss.any() <= 1e-7: return src_img    \n",
    "    result = st * (src_img.astype(np.float32) - ms) / (ss+1e-7) + mt\n",
    "    result = (255.0/result.max()*result).astype(np.float32)\n",
    "    return result\n",
    "\n",
    "def transform_face_video_conv(model, inp_img, direction=\"A2B\", roi_coef=15, color_correction=False):\n",
    "    def get_feather_edges_mask(img, roi_coef):\n",
    "        img_size = img.shape\n",
    "        mask = np.zeros_like(img)\n",
    "        mask[img_size[0]//roi_coef:-img_size[0]//roi_coef, \n",
    "             img_size[1]//roi_coef:-img_size[1]//roi_coef,:]  = 255\n",
    "        mask = cv2.GaussianBlur(mask,(15,15),10)\n",
    "        return mask        \n",
    "\n",
    "    if direction == \"AtoB\":\n",
    "        path_func = model.path_abgr_B\n",
    "    elif direction == \"BtoA\":\n",
    "        path_func = model.path_abgr_A\n",
    "    else:\n",
    "        raise ValueError(f\"direction should be either AtoB or BtoA, recieved {direction}.\")\n",
    "\n",
    "    # pre-process input image\n",
    "    img_bgr = cv2.cvtColor(inp_img, cv2.COLOR_RGB2BGR)\n",
    "    input_size = img_bgr.shape        \n",
    "    roi = img_bgr[input_size[0]//roi_coef:-input_size[0]//roi_coef, \n",
    "                  input_size[1]//roi_coef:-input_size[1]//roi_coef,:] # BGR, [0, 255]  \n",
    "    roi_size = roi.shape\n",
    "    ae_input = cv2.resize(roi, model.IMAGE_SHAPE[:2])/255. * 2 - 1 # BGR, [-1, 1]       \n",
    "\n",
    "    # post-process transformed roi image\n",
    "    ae_output = np.squeeze(np.array([path_func([[ae_input]])]))\n",
    "    ae_output_a = ae_output[:,:,0] * 255\n",
    "    ae_output_a = cv2.resize(ae_output_a, (roi_size[1],roi_size[0]))[...,np.newaxis]\n",
    "    ae_output_bgr = np.clip( (ae_output[:,:,1:] + 1) * 255 / 2, 0, 255)\n",
    "    ae_output_bgr = cv2.resize(ae_output_bgr, (roi_size[1],roi_size[0]))\n",
    "    ae_output_masked = (ae_output_a/255 * ae_output_bgr + (1 - ae_output_a/255) * roi).astype('uint8') # BGR, [0, 255]\n",
    "    if color_correction == \"adain\":\n",
    "        ae_output_masked = shift_n_scale(ae_output_masked, roi)\n",
    "        ae_output_bgr = shift_n_scale(ae_output_bgr, roi)\n",
    "    elif color_correction == \"hist_match\":\n",
    "        ae_output_masked = color_hist_match(ae_output_masked, roi)\n",
    "        ae_output_bgr = color_hist_match(ae_output_bgr, roi)\n",
    "\n",
    "    # merge transformed output back to input image\n",
    "    blend_mask = get_feather_edges_mask(roi, roi_coef)        \n",
    "    blended_img = blend_mask/255 * ae_output_masked + (1-blend_mask/255) * roi\n",
    "    result = img_bgr.copy()\n",
    "    result[input_size[0]//roi_coef:-input_size[0]//roi_coef, \\\n",
    "           input_size[1]//roi_coef:-input_size[1]//roi_coef,:] = blended_img \n",
    "    result_rawRGB = img_bgr.copy()\n",
    "    result_rawRGB[input_size[0]//roi_coef:-input_size[0]//roi_coef, \\\n",
    "                  input_size[1]//roi_coef:-input_size[1]//roi_coef,:] = ae_output_bgr \n",
    "    result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB) \n",
    "    result_rawRGB = cv2.cvtColor(result_rawRGB, cv2.COLOR_BGR2RGB)\n",
    "    result_alpha = np.zeros_like(img_bgr)\n",
    "    result_alpha[input_size[0]//roi_coef:-input_size[0]//roi_coef, \\\n",
    "                 input_size[1]//roi_coef:-input_size[1]//roi_coef,:] = (blend_mask/255) * ae_output_a \n",
    "    return result, result_rawRGB, result_alpha\n",
    "\n",
    "def is_overlap(box1, box2):\n",
    "    overlap_x0 = np.max([box1[0], box2[0]]).astype(np.float32)\n",
    "    overlap_y1 = np.min([box1[1], box2[1]]).astype(np.float32)\n",
    "    overlap_x1 = np.min([box1[2], box2[2]]).astype(np.float32)\n",
    "    overlap_y0 = np.max([box1[3], box2[3]]).astype(np.float32)\n",
    "    area_iou = (overlap_x1-overlap_x0) * (overlap_y1-overlap_y0)\n",
    "    area_box1 = (box1[2]-box1[0]) * (box1[1]-box1[3])\n",
    "    area_box2 = (box2[2]-box2[0]) * (box2[1]-box2[3])    \n",
    "    return (area_iou / area_box1) >= 0.2\n",
    "    \n",
    "def remove_overlaps(faces):    \n",
    "    main_face = get_most_conf_face(faces)\n",
    "    main_face_bbox = main_face[0]\n",
    "    result_faces = []\n",
    "    result_faces.append(main_face_bbox)\n",
    "    for (x0, y1, x1, y0, conf_score) in faces:\n",
    "        if not is_overlap(main_face_bbox, (x0, y1, x1, y0)):\n",
    "            result_faces.append((x0, y1, x1, y0, conf_score))\n",
    "    return result_faces\n",
    "\n",
    "def get_most_conf_face(faces):\n",
    "    # Return the bbox w/ the highest confidence score\n",
    "    best_conf_score = 0\n",
    "    conf_face = None\n",
    "    for (x0, y1, x1, y0, conf_score) in faces: \n",
    "        if conf_score >= best_conf_score:\n",
    "            best_conf_score = conf_score\n",
    "            conf_face = [(x0, y1, x1, y0, conf_score)]\n",
    "    return conf_face\n",
    "\n",
    "def kalmanfilter_init(noise_coef):\n",
    "    kf = cv2.KalmanFilter(4,2)\n",
    "    kf.measurementMatrix = np.array([[1,0,0,0],[0,1,0,0]], np.float32)\n",
    "    kf.transitionMatrix = np.array([[1,0,1,0],[0,1,0,1],[0,0,1,0],[0,0,0,1]], np.float32)\n",
    "    kf.processNoiseCov = noise_coef * np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]], np.float32)\n",
    "    return kf\n",
    "\n",
    "def is_higher_than_480p(x):\n",
    "    return (x.shape[0] * x.shape[1]) >= (858*480)\n",
    "\n",
    "def is_higher_than_720p(x):\n",
    "    return (x.shape[0] * x.shape[1]) >= (1280*720)\n",
    "\n",
    "def is_higher_than_1080p(x):\n",
    "    return (x.shape[0] * x.shape[1]) >= (1920*1080)\n",
    "\n",
    "def calibrate_coord(faces, video_scaling_factor):\n",
    "    for i, (x0, y1, x1, y0, _) in enumerate(faces):\n",
    "        faces[i] = (x0*video_scaling_factor, y1*video_scaling_factor, \n",
    "                    x1*video_scaling_factor, y0*video_scaling_factor, _)\n",
    "    return faces\n",
    "\n",
    "def calibrate_landmarks(pnts, video_scaling_factor):\n",
    "    for i, xy in enumerate(pnts):\n",
    "        pnts[i] = xy*video_scaling_factor\n",
    "    return pnts\n",
    "\n",
    "def process_mtcnn_bbox(bboxes, im_shape):\n",
    "    # output bbox coordinate of MTCNN is (y0, x0, y1, x1)\n",
    "    # Here we process the bbox coord. to a square bbox with ordering (x0, y1, x1, y0)\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        y0, x0, y1, x1 = bboxes[i,0:4]\n",
    "        w = int(y1 - y0)\n",
    "        h = int(x1 - x0)\n",
    "        length = (w + h)/2\n",
    "        center = (int((x1+x0)/2),int((y1+y0)/2))\n",
    "        new_x0 = np.max([0, (center[0]-length//2)])#.astype(np.int32)\n",
    "        new_x1 = np.min([im_shape[0], (center[0]+length//2)])#.astype(np.int32)\n",
    "        new_y0 = np.max([0, (center[1]-length//2)])#.astype(np.int32)\n",
    "        new_y1 = np.min([im_shape[1], (center[1]+length//2)])#.astype(np.int32)\n",
    "        bboxes[i,0:4] = new_x0, new_y1, new_x1, new_y0\n",
    "    return bboxes\n",
    "\n",
    "def get_faces_bbox(image):  \n",
    "    global pnet, rnet, onet \n",
    "    global detec_threshold\n",
    "    minsize = 20 # minimum size of face\n",
    "    threshold = [ 0.6, 0.7, detec_threshold ]  # three steps's threshold\n",
    "    factor = 0.709 # scale factor\n",
    "    if manually_downscale:\n",
    "        video_scaling_factor = manually_downscale_factor\n",
    "        resized_image = cv2.resize(image, \n",
    "                                   (image.shape[1]//video_scaling_factor, \n",
    "                                    image.shape[0]//video_scaling_factor))\n",
    "        faces, pnts = mtcnn_detect_face.detect_face(resized_image, minsize, pnet, rnet, onet, threshold, factor)\n",
    "        faces = process_mtcnn_bbox(faces, resized_image.shape)\n",
    "        faces = calibrate_coord(faces, video_scaling_factor)\n",
    "        pnts = calibrate_landmarks(pnts, video_scaling_factor)\n",
    "    elif is_higher_than_1080p(image):\n",
    "        video_scaling_factor = 4 + video_scaling_offset\n",
    "        resized_image = cv2.resize(image, \n",
    "                                   (image.shape[1]//video_scaling_factor, \n",
    "                                    image.shape[0]//video_scaling_factor))\n",
    "        faces, pnts = mtcnn_detect_face.detect_face(resized_image, minsize, pnet, rnet, onet, threshold, factor)\n",
    "        faces = process_mtcnn_bbox(faces, resized_image.shape)\n",
    "        faces = calibrate_coord(faces, video_scaling_factor)\n",
    "        pnts = calibrate_landmarks(pnts, video_scaling_factor)\n",
    "    elif is_higher_than_720p(image):\n",
    "        video_scaling_factor = 3 + video_scaling_offset\n",
    "        resized_image = cv2.resize(image, \n",
    "                                   (image.shape[1]//video_scaling_factor, \n",
    "                                    image.shape[0]//video_scaling_factor))\n",
    "        faces, pnts = mtcnn_detect_face.detect_face(resized_image, minsize, pnet, rnet, onet, threshold, factor)\n",
    "        faces = process_mtcnn_bbox(faces, resized_image.shape)\n",
    "        faces = calibrate_coord(faces, video_scaling_factor)  \n",
    "        pnts = calibrate_landmarks(pnts, video_scaling_factor)\n",
    "    elif is_higher_than_480p(image):\n",
    "        video_scaling_factor = 2 + video_scaling_offset\n",
    "        resized_image = cv2.resize(image, \n",
    "                                   (image.shape[1]//video_scaling_factor, \n",
    "                                    image.shape[0]//video_scaling_factor))\n",
    "        faces, pnts = mtcnn_detect_face.detect_face(resized_image, minsize, pnet, rnet, onet, threshold, factor)\n",
    "        faces = process_mtcnn_bbox(faces, resized_image.shape)\n",
    "        faces = calibrate_coord(faces, video_scaling_factor)\n",
    "        pnts = calibrate_landmarks(pnts, video_scaling_factor)\n",
    "    else:\n",
    "        faces, pnts = mtcnn_detect_face.detect_face(image, minsize, pnet, rnet, onet, threshold, factor)\n",
    "        faces = process_mtcnn_bbox(faces, image.shape)\n",
    "    return faces, pnts\n",
    "\n",
    "def get_smoothed_coord(x0, x1, y0, y1, shape, ratio=0.65):\n",
    "    global prev_x0, prev_x1, prev_y0, prev_y1\n",
    "    if not use_kalman_filter:\n",
    "        x0 = int(ratio * prev_x0 + (1-ratio) * x0)\n",
    "        x1 = int(ratio * prev_x1 + (1-ratio) * x1)\n",
    "        y1 = int(ratio * prev_y1 + (1-ratio) * y1)\n",
    "        y0 = int(ratio * prev_y0 + (1-ratio) * y0)\n",
    "    else:\n",
    "        x0y0 = np.array([x0, y0]).astype(np.float32)\n",
    "        x1y1 = np.array([x1, y1]).astype(np.float32)\n",
    "        kf0.correct(x0y0)\n",
    "        pred_x0y0 = kf0.predict()\n",
    "        kf1.correct(x1y1)\n",
    "        pred_x1y1 = kf1.predict()\n",
    "        x0 = np.max([0, pred_x0y0[0][0]]).astype(np.int)\n",
    "        x1 = np.min([shape[0], pred_x1y1[0][0]]).astype(np.int)\n",
    "        y0 = np.max([0, pred_x0y0[1][0]]).astype(np.int)\n",
    "        y1 = np.min([shape[1], pred_x1y1[1][0]]).astype(np.int)\n",
    "        if x0 == x1 or y0 == y1:\n",
    "            x0, y0, x1, y1 = prev_x0, prev_y0, prev_x1, prev_y1\n",
    "    return x0, x1, y0, y1     \n",
    "    \n",
    "def set_global_coord(x0, x1, y0, y1):\n",
    "    global prev_x0, prev_x1, prev_y0, prev_y1\n",
    "    prev_x0 = x0\n",
    "    prev_x1 = x1\n",
    "    prev_y1 = y1\n",
    "    prev_y0 = y0\n",
    "\"\"\"    \n",
    "def generate_face(ae_input, path_abgr, roi_size, roi_image):\n",
    "    result = np.squeeze(np.array([path_abgr([[ae_input]])]))\n",
    "    result_a = result[:,:,0] * 255\n",
    "    result_a = cv2.GaussianBlur(result_a, (7,7), 6)\n",
    "    result_a = cv2.resize(result_a, (roi_size[1],roi_size[0]))[...,np.newaxis]\n",
    "    result_bgr = np.clip((result[:,:,1:] + 1) * 255 / 2, 0, 255)\n",
    "    result_bgr = cv2.resize(result_bgr, (roi_size[1],roi_size[0]))\n",
    "    result = (result_a/255 * result_bgr + (1 - result_a/255) * roi_image).astype(np.uint8)\n",
    "    result = cv2.cvtColor(result.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "    if use_color_correction:\n",
    "        result = adain(result, roi_image)\n",
    "    return result.astype(np.uint8), result_a.astype(np.uint8)\n",
    "\"\"\"\n",
    "def get_init_mask_map(image):\n",
    "    return np.zeros_like(image)\n",
    "\n",
    "def get_init_comb_img(input_img):\n",
    "    comb_img = np.zeros([input_img.shape[0], input_img.shape[1]*2,input_img.shape[2]])\n",
    "    comb_img[:, :input_img.shape[1], :] = input_img\n",
    "    comb_img[:, input_img.shape[1]:, :] = input_img\n",
    "    return comb_img    \n",
    "\n",
    "def get_init_triple_img(input_img, no_face=False):\n",
    "    if no_face:\n",
    "        triple_img = np.zeros([input_img.shape[0], input_img.shape[1]*3,input_img.shape[2]])\n",
    "        triple_img[:, :input_img.shape[1], :] = input_img\n",
    "        triple_img[:, input_img.shape[1]:input_img.shape[1]*2, :] = input_img      \n",
    "        triple_img[:, input_img.shape[1]*2:, :] = (input_img * .15).astype('uint8')  \n",
    "        return triple_img\n",
    "    else:\n",
    "        triple_img = np.zeros([input_img.shape[0], input_img.shape[1]*3,input_img.shape[2]])\n",
    "        return triple_img\n",
    "\n",
    "def get_mask(roi_image, h, w):\n",
    "    mask = np.zeros_like(roi_image)\n",
    "    mask[h//15:-h//15,w//15:-w//15,:] = 255\n",
    "    mask = cv2.GaussianBlur(mask,(15,15),10)\n",
    "    return mask\n",
    "\n",
    "\"\"\" Color corretion functions\"\"\"\n",
    "def hist_match(source, template):\n",
    "    # Code borrow from:\n",
    "    # https://stackoverflow.com/questions/32655686/histogram-matching-of-two-images-in-python-2-x\n",
    "    oldshape = source.shape\n",
    "    source = source.ravel()\n",
    "    template = template.ravel()\n",
    "    s_values, bin_idx, s_counts = np.unique(source, return_inverse=True,\n",
    "                                            return_counts=True)\n",
    "    t_values, t_counts = np.unique(template, return_counts=True)\n",
    "\n",
    "    s_quantiles = np.cumsum(s_counts).astype(np.float64)\n",
    "    s_quantiles /= s_quantiles[-1]\n",
    "    t_quantiles = np.cumsum(t_counts).astype(np.float64)\n",
    "    t_quantiles /= t_quantiles[-1]\n",
    "    interp_t_values = np.interp(s_quantiles, t_quantiles, t_values)\n",
    "\n",
    "    return interp_t_values[bin_idx].reshape(oldshape)\n",
    "\n",
    "def color_hist_match(src_im, tar_im):\n",
    "    #src_im = cv2.cvtColor(src_im, cv2.COLOR_BGR2Lab)\n",
    "    #tar_im = cv2.cvtColor(tar_im, cv2.COLOR_BGR2Lab)\n",
    "    matched_R = hist_match(src_im[:,:,0], tar_im[:,:,0])\n",
    "    matched_G = hist_match(src_im[:,:,1], tar_im[:,:,1])\n",
    "    matched_B = hist_match(src_im[:,:,2], tar_im[:,:,2])\n",
    "    matched = np.stack((matched_R, matched_G, matched_B), axis=2).astype(np.float64)\n",
    "    return matched\n",
    "\n",
    "def adain(src_im, tar_im):\n",
    "    # https://github.com/ftokarev/tf-adain/blob/master/adain/norm.py\n",
    "    mt = np.mean(tar_im, axis=(0,1))\n",
    "    st = np.std(tar_im, axis=(0,1))\n",
    "    ms = np.mean(src_im, axis=(0,1))\n",
    "    ss = np.std(src_im, axis=(0,1))    \n",
    "    if ss.any() <= 1e-7: return src_im    \n",
    "    result = st * (src_im.astype(np.float32) - ms) / (ss+1e-7) + mt\n",
    "    result = (255.0/result.max()*result).astype(np.float32)\n",
    "    return result\n",
    "\n",
    "def process_video(input_img): \n",
    "    global prev_x0, prev_x1, prev_y0, prev_y1\n",
    "    global frames      \n",
    "    global pnet, rnet, onet\n",
    "    global roi_coef\n",
    "    global direction\n",
    "    \"\"\"\n",
    "    # The following if statement is meant to solve a bug that has an unknow cause.\n",
    "    # (No such bug using tensorflow-cpu)\n",
    "    if frames <= -1:\n",
    "        with tf.variable_scope('pnet2', reuse=True):\n",
    "            pnet2 = None\n",
    "            data = tf.placeholder(tf.float32, (None,None,None,3), 'input')\n",
    "            pnet2 = mtcnn_detect_face.PNet({'data':data})\n",
    "            pnet2.load(os.path.join(\"./mtcnn_weights/\", 'det1.npy'), sess)\n",
    "        with tf.variable_scope('rnet2', reuse=True):\n",
    "            rnet2 = None\n",
    "            data = tf.placeholder(tf.float32, (None,24,24,3), 'input')\n",
    "            rnet2 = mtcnn_detect_face.RNet({'data':data})\n",
    "            rnet2.load(os.path.join(\"./mtcnn_weights/\", 'det2.npy'), sess)\n",
    "        with tf.variable_scope('onet2', reuse=True):\n",
    "            onet2 = None\n",
    "            data = tf.placeholder(tf.float32, (None,48,48,3), 'input')\n",
    "            onet2 = mtcnn_detect_face.ONet({'data':data})\n",
    "            onet2.load(os.path.join(\"./mtcnn_weights/\", 'det3.npy'), sess)\n",
    "        pnet = K.function([pnet2.layers['data']],\n",
    "                          [pnet2.layers['conv4-2'], \n",
    "                           pnet2.layers['prob1']])\n",
    "        rnet = K.function([rnet2.layers['data']],\n",
    "                          [rnet2.layers['conv5-2'], \n",
    "                           rnet2.layers['prob1']])\n",
    "        onet = K.function([onet2.layers['data']], \n",
    "                          [onet2.layers['conv6-2'], \n",
    "                           onet2.layers['conv6-3'], \n",
    "                           onet2.layers['prob1']])\n",
    "    \"\"\"\n",
    "    \n",
    "    image = input_img\n",
    "    \n",
    "    # detect face using MTCNN\n",
    "    # faces: face bbox coord, pnts: landmarks coord.\n",
    "    faces, pnts = get_faces_bbox(image)\n",
    "    \n",
    "    # check if any face detected\n",
    "    if len(faces) == 0:\n",
    "        comb_img = get_init_comb_img(input_img)\n",
    "        triple_img = get_init_triple_img(input_img, no_face=True)\n",
    "    else:\n",
    "        faces = remove_overlaps(faces) # Has non-max suppress already been implemented in MTCNN?\n",
    "    \n",
    "    # init. output image\n",
    "    mask_map = get_init_mask_map(image)\n",
    "    comb_img = get_init_comb_img(input_img)\n",
    "    best_conf_score = 0\n",
    "    \n",
    "    # loop through all detected faces\n",
    "    for (x0, y1, x1, y0, conf_score) in faces:    \n",
    "        # smoothe the bounding box\n",
    "        if use_smoothed_bbox:\n",
    "            if frames != 0 and conf_score >= best_conf_score:\n",
    "                x0, x1, y0, y1 = get_smoothed_coord(x0, x1, y0, y1, \n",
    "                                                    image.shape, \n",
    "                                                    ratio=0.65 if use_kalman_filter else bbox_moving_avg_coef)\n",
    "                set_global_coord(x0, x1, y0, y1)\n",
    "                best_conf_score = conf_score\n",
    "                frames += 1\n",
    "            elif conf_score <= best_conf_score:\n",
    "                frames += 1\n",
    "            else:\n",
    "                if conf_score >= best_conf_score:\n",
    "                    set_global_coord(x0, x1, y0, y1)\n",
    "                    best_conf_score = conf_score\n",
    "                if use_kalman_filter:\n",
    "                    for i in range(200):\n",
    "                        kf0.predict()\n",
    "                        kf1.predict()\n",
    "                frames += 1\n",
    "        \n",
    "        # transform face\n",
    "        try:\n",
    "            # get detected face\n",
    "            det_face_im = input_img[int(x0):int(x1),int(y0):int(y1),:]\n",
    "\n",
    "            # get src/tar landmarks\n",
    "            src_landmarks = get_src_landmarks(x0, x1, y0, y1, pnts)\n",
    "            tar_landmarks = get_tar_landmarks(det_face_im)\n",
    "\n",
    "            # align detected face\n",
    "            aligned_det_face_im = landmarks_match_mtcnn(det_face_im, src_landmarks, tar_landmarks)\n",
    "\n",
    "            # face transform\n",
    "            r_im, r_rgb, r_a = transform_face_video_conv(\n",
    "                model,\n",
    "                aligned_det_face_im, \n",
    "                direction=direction, \n",
    "                roi_coef=roi_coef,\n",
    "                color_correction=use_color_correction\n",
    "                )\n",
    "\n",
    "            # reverse alignment\n",
    "            rev_aligned_det_face_im = landmarks_match_mtcnn(r_rgb, tar_landmarks, src_landmarks)\n",
    "            rev_aligned_mask = landmarks_match_mtcnn(r_a, tar_landmarks, src_landmarks)\n",
    "\n",
    "            # merge source face and transformed face\n",
    "            result = np.zeros_like(det_face_im)\n",
    "            result = rev_aligned_mask/255*rev_aligned_det_face_im + (1-rev_aligned_mask/255)*det_face_im\n",
    "            result_a = rev_aligned_mask\n",
    "        except:            \n",
    "            # catch exceptions for landmarks alignment errors (if any)\n",
    "            print(f\"Face alignment error occurs at frame {frames}.\")\n",
    "            result, _, result_a = transform_face_video_conv(\n",
    "                model,\n",
    "                input_img[int(x0):int(x1),int(y0):int(y1),:],\n",
    "                direction=direction, \n",
    "                roi_coef=roi_coef,\n",
    "                color_correction=use_color_correction\n",
    "                )\n",
    "        \n",
    "        comb_img[int(x0):int(x1),input_img.shape[1]+int(y0):input_img.shape[1]+int(y1),:] = result\n",
    "        \n",
    "        if conf_score >= best_conf_score:\n",
    "            mask_map[int(x0):int(x1),int(y0):int(y1),:] = result_a\n",
    "            mask_map = np.clip(mask_map + .15 * input_img, 0, 255)     \n",
    "        else:\n",
    "            mask_map[int(x0):int(x1),int(y0):int(y1),:] += result_a\n",
    "            mask_map = np.clip(mask_map, 0, 255)\n",
    "            \n",
    "        triple_img = get_init_triple_img(input_img)\n",
    "        triple_img[:, :input_img.shape[1]*2, :] = comb_img\n",
    "        triple_img[:, input_img.shape[1]*2:, :] = mask_map\n",
    "        \n",
    "        # Draw bbox\n",
    "        #cv2.rectangle(triple_img,(int(y0),int(x0)),(int(y1),int(x1)),(0,255,125),3\n",
    "        \n",
    "    global output_type\n",
    "    if output_type == 1:\n",
    "        return comb_img[:, input_img.shape[1]:, :]  # return only result image\n",
    "    elif output_type == 2:\n",
    "        return comb_img  # return input and result image combined as one\n",
    "    elif output_type == 3:\n",
    "        return triple_img #return input,result and mask heatmap image combined as one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video conversion configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_kalman_filter = True\n",
    "if use_kalman_filter:\n",
    "    noise_coef = 5e-3 # Increase by 10x if tracking is slow. \n",
    "    kf0 = kalmanfilter_init(noise_coef)\n",
    "    kf1 = kalmanfilter_init(noise_coef)\n",
    "else:\n",
    "    bbox_moving_avg_coef = 0.65\n",
    "    \n",
    "video_scaling_offset = 0 # Increase by 1 if OOM happens.\n",
    "manually_downscale = False\n",
    "manually_downscale_factor = int(2) # should be an positive integer\n",
    "use_color_correction = \"adain\" # none, adain, hist_match\n",
    "\n",
    "# Output type: \n",
    "#    1. [ result ] \n",
    "#    2. [ source | result ] \n",
    "#    3. [ source | result | mask ]\n",
    "global output_type\n",
    "output_type = 3\n",
    "\n",
    "# Detection threshold:  a float point between 0 and 1. Decrease this value if faces are missed.\n",
    "global detec_threshold\n",
    "detec_threshold = 0.7\n",
    "\n",
    "# Center area of input images to be cropped into\n",
    "# Suggest value: 5 ~ 25\n",
    "global roi_coef\n",
    "roi_coef = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set transform direciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global direction\n",
    "direction = \"AtoB\" # default trainsforming faceA to faceB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start video conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for smoothing bounding box\n",
    "global prev_x0, prev_x1, prev_y0, prev_y1\n",
    "global frames\n",
    "global prev_pnts1, prev_pnts2\n",
    "prev_x0 = prev_x1 = prev_y0 = prev_y1 = 0\n",
    "frames = 0\n",
    "prev_pnts1 = prev_pnts2 = np.array([])\n",
    "\n",
    "output = 'OUTPUT_VIDEO.mp4'\n",
    "clip1 = VideoFileClip(\"INPUT_VIDEO.mp4\")\n",
    "clip = clip1.fl_image(process_video)#.subclip(7.5, 9) #NOTE: this function expects color images!!\n",
    "%time clip.write_videofile(output, audio=False)\n",
    "clip1.reader.close()\n",
    "try:\n",
    "    clip1.audio.reader.close_proc()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
