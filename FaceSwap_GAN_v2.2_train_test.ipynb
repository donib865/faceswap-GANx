{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umeyama import umeyama\n",
    "from image_augmentation import random_transform\n",
    "from prefetch_generator import background # !pip install prefetch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from pathlib import PurePath, Path\n",
    "from random import randint, shuffle\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_learning_phase(1)\n",
    "# K.set_learning_phase(0) # set to 0 in inference phase (video conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input/Output resolution\n",
    "RESOLUTION = 64 # 64x64, 128x128, 256x256\n",
    "assert (RESOLUTION % 64) == 0, \"RESOLUTION should be 64, 128, or 256.\"\n",
    "\n",
    "# Batch size\n",
    "batchSize = 8\n",
    "\n",
    "# Use motion blurs (data augmentation)\n",
    "# set True if training data contains images extracted from videos\n",
    "use_da_motion_blur = False \n",
    "\n",
    "# Use eye-aware training\n",
    "# require images generated from prep_binary_masks.ipynb\n",
    "use_bm_eyes = True\n",
    "\n",
    "# Probability of random color matching (data augmentation)\n",
    "prob_random_color_match = 0.5\n",
    "\n",
    "# Path to training images\n",
    "img_dirA = './faceA'\n",
    "img_dirB = './faceB'\n",
    "img_dirA_bm_eyes = \"./binary_masks/faceA_eyes\"\n",
    "img_dirB_bm_eyes = \"./binary_masks/faceB_eyes\"\n",
    "\n",
    "# Path to saved model weights\n",
    "models_dir = \"./models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture configuration\n",
    "arch_config = {}\n",
    "arch_config['IMAGE_SHAPE'] = (RESOLUTION, RESOLUTION, 3)\n",
    "arch_config['use_self_attn'] = True\n",
    "arch_config['norm'] = \"instancenorm\" # instancenorm, batchnorm, layernorm, groupnorm, none\n",
    "arch_config['model_capacity'] = \"standard\" # standard, lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function weights configuration\n",
    "loss_weights = {}\n",
    "loss_weights['w_D'] = 0.1 # Discriminator\n",
    "loss_weights['w_recon'] = 1. # L1 reconstruction loss\n",
    "loss_weights['w_edge'] = 0.1 # edge loss\n",
    "loss_weights['w_eyes'] = 30. # reconstruction and edge loss on eyes area\n",
    "loss_weights['w_pl'] = (0.01, 0.1, 0.3, 0.1) # perceptual loss (0.003, 0.03, 0.3, 0.3)\n",
    "\n",
    "# Init. loss config.\n",
    "loss_config = {}\n",
    "loss_config[\"gan_training\"] = \"mixup_LSGAN\" # \"mixup_LSGAN\" or \"relativistic_avg_LSGAN\"\n",
    "loss_config['use_PL'] = False\n",
    "loss_config['use_mask_hinge_loss'] = False\n",
    "loss_config['m_mask'] = 0.\n",
    "loss_config['lr_factor'] = 1.\n",
    "loss_config['use_cyclic_loss'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "# Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.faceswap_gan_model import FaceswapGANModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = FaceswapGANModel(**arch_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "# Load Model Weights\n",
    "\n",
    "Weights file names:\n",
    "```python\n",
    "encoder.h5, decoder_A.h5, deocder_B.h5, netDA.h5, netDB.h5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights files are successfully loaded\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(path=models_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The following cells are for training, skip to [transform_face()](#tf) for inference.\n",
    "\n",
    "# Define Losses and Build Training Functions\n",
    "\n",
    "TODO: split into two methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/rcmalli/keras-vggface\n",
    "#!pip install keras_vggface\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "# VGGFace ResNet50\n",
    "vggface = VGGFace(include_top=False, model='resnet50', input_shape=(224, 224, 3))\n",
    "\n",
    "#vggface.summary()\n",
    "\n",
    "model.build_pl_model(vggface_model=vggface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_train_functions(loss_weights=loss_weights, **loss_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='9'></a>\n",
    "# DataLoader\n",
    "\n",
    "TODO: write a DataLoader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motion blurs as data augmentation\n",
    "def get_motion_blur_kernel(sz=7):\n",
    "    rot_angle = np.random.uniform(-180,180)\n",
    "    kernel = np.zeros((sz,sz))\n",
    "    kernel[int((sz-1)//2), :] = np.ones(sz)\n",
    "    kernel = ndimage.interpolation.rotate(kernel, rot_angle, reshape=False)\n",
    "    kernel = np.clip(kernel, 0, 1)\n",
    "    normalize_factor = 1 / np.sum(kernel)\n",
    "    kernel = kernel * normalize_factor\n",
    "    return kernel\n",
    "\n",
    "def motion_blur(images, sz=7):\n",
    "    # images is a list [image2, image2, ...]\n",
    "    blur_sz = np.random.choice([5, 7, 9, 11])\n",
    "    kernel_motion_blur = get_motion_blur_kernel(blur_sz)\n",
    "    for i, image in enumerate(images):\n",
    "        images[i] = cv2.filter2D(image, -1, kernel_motion_blur).astype(np.float64)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils for loading data\n",
    "def load_data(file_pattern):\n",
    "    return glob.glob(file_pattern)\n",
    "  \n",
    "def random_warp_rev(image, res=RESOLUTION):\n",
    "    assert image.shape == (256,256,6)\n",
    "    res_scale = res//64\n",
    "    assert res_scale >= 1, f\"Resolution should be >= 64. Recieved {res}.\"\n",
    "    interp_param = 80 * res_scale\n",
    "    interp_slice = slice(interp_param//10,9*interp_param//10)\n",
    "    dst_pnts_slice = slice(0,65*res_scale,16*res_scale)\n",
    "    \n",
    "    rand_coverage = np.random.randint(25) + 80 # random warping coverage\n",
    "    rand_scale = np.random.uniform(5., 6.2) # random warping scale\n",
    "    \n",
    "    range_ = np.linspace(128-rand_coverage, 128+rand_coverage, 5)\n",
    "    mapx = np.broadcast_to(range_, (5,5))\n",
    "    mapy = mapx.T\n",
    "    mapx = mapx + np.random.normal(size=(5,5), scale=rand_scale)\n",
    "    mapy = mapy + np.random.normal(size=(5,5), scale=rand_scale)\n",
    "    interp_mapx = cv2.resize(mapx, (interp_param,interp_param))[interp_slice,interp_slice].astype('float32')\n",
    "    interp_mapy = cv2.resize(mapy, (interp_param,interp_param))[interp_slice,interp_slice].astype('float32')\n",
    "    warped_image = cv2.remap(image, interp_mapx, interp_mapy, cv2.INTER_LINEAR)\n",
    "    src_points = np.stack([mapx.ravel(), mapy.ravel()], axis=-1)\n",
    "    dst_points = np.mgrid[dst_pnts_slice,dst_pnts_slice].T.reshape(-1,2)\n",
    "    mat = umeyama(src_points, dst_points, True)[0:2]\n",
    "    target_image = cv2.warpAffine(image, mat, (res,res))\n",
    "    return warped_image, target_image\n",
    "\n",
    "def random_color_match(image):\n",
    "    global fns_all_trn_data\n",
    "    rand_idx = np.random.randint(len(fns_all_trn_data))\n",
    "    fn_match = fns_all_trn_data[rand_idx]\n",
    "    tar_img = cv2.imread(fn_match)\n",
    "    if tar_img is None:\n",
    "        print(f\"Failed reading image {fn_match} in random_color_match().\")\n",
    "        return image\n",
    "    r = 60\n",
    "    src_img = cv2.resize(image, (256,256))\n",
    "    tar_img = cv2.resize(tar_img, (256,256))\n",
    "    mt = np.mean(tar_img[r:-r,r:-r,:], axis=(0,1))\n",
    "    st = np.std(tar_img[r:-r,r:-r,:], axis=(0,1))\n",
    "    ms = np.mean(src_img[r:-r,r:-r,:], axis=(0,1))\n",
    "    ss = np.std(src_img[r:-r,r:-r,:], axis=(0,1))    \n",
    "    if ss.any() <= 1e-7: return src_img    \n",
    "    result = st * (src_img.astype(np.float32) - ms) / (ss+1e-7) + mt\n",
    "    if result.min() < 0:\n",
    "        result = result - result.min()\n",
    "    if result.max() > 255:\n",
    "        result = (255.0/result.max()*result).astype(np.float32)\n",
    "    return result\n",
    "\n",
    "random_transform_args = {\n",
    "    'rotation_range': 10,\n",
    "    'zoom_range': 0.1,\n",
    "    'shift_range': 0.05,\n",
    "    'random_flip': 0.5,\n",
    "    }\n",
    "def read_image(fn, dir_bm_eyes=None, random_transform_args=random_transform_args):\n",
    "    if dir_bm_eyes is None: raise ValueError(f\"dir_bm_eyes is not set.\")\n",
    "        \n",
    "    raw_fn = PurePath(fn).parts[-1]\n",
    "    image = cv2.imread(fn)\n",
    "    if image is None: raise IOError(f\"Failed reading image {fn}.\")        \n",
    "    if np.random.uniform() <= prob_random_color_match:\n",
    "        image = random_color_match(image)\n",
    "    image = cv2.resize(image, (256,256)) / 255 * 2 - 1\n",
    "    \n",
    "    if use_bm_eyes:\n",
    "        bm_eyes = cv2.imread(f\"{dir_bm_eyes}/{raw_fn}\")\n",
    "        if bm_eyes is None:\n",
    "            raise IOError(f\"Failed reading binary mask {dir_bm_eyes}/{raw_fn}.\")\n",
    "        bm_eyes = cv2.resize(bm_eyes, (256,256)) / 255.\n",
    "    else:\n",
    "        bm_eyes = np.zeros_like(image)\n",
    "    \n",
    "    image = np.concatenate([image, bm_eyes], axis=-1)\n",
    "    image = random_transform(image, **random_transform_args)\n",
    "    warped_img, target_img = random_warp_rev(image)\n",
    "    \n",
    "    bm_eyes = target_img[...,3:]\n",
    "    warped_img = warped_img[...,:3]\n",
    "    target_img = target_img[...,:3]\n",
    "    \n",
    "    # Motion blur data augmentation:\n",
    "    # we want the model to learn to preserve motion blurs of input images\n",
    "    if np.random.uniform() < 0.25 and use_da_motion_blur: \n",
    "        warped_img, target_img = motion_blur([warped_img, target_img])\n",
    "    \n",
    "    return warped_img, target_img, bm_eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A generator function that yields epoch and data\n",
    "@background(32)\n",
    "def minibatch(data, batchsize, dir_bm_eyes):\n",
    "    length = len(data)\n",
    "    epoch = i = 0\n",
    "    tmpsize = None  \n",
    "    shuffle(data)\n",
    "    while True:\n",
    "        size = tmpsize if tmpsize else batchsize\n",
    "        if i+size > length:\n",
    "            shuffle(data)\n",
    "            i = 0\n",
    "            epoch+=1        \n",
    "        rtn = np.float32([read_image(data[j], dir_bm_eyes) for j in range(i,i+size)])\n",
    "        i+=size\n",
    "        tmpsize = yield epoch, rtn[:,0,:,:,:], rtn[:,1,:,:,:], rtn[:,2,:,:,:]       \n",
    "\n",
    "def create_minibatch(data, batchsize, dir_bm_eyes):\n",
    "    # This is a redundant function, to be written in to a DataLoader class.\n",
    "    batch = minibatch(data, batchsize, dir_bm_eyes)\n",
    "    tmpsize = None    \n",
    "    while True:        \n",
    "        ep1, warped_img, target_img, bm_eyes = next(batch)\n",
    "        tmpsize = yield ep1, warped_img, target_img, bm_eyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizer\n",
    "\n",
    "TODO: write a Visualizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import showG, showG_mask, showG_eyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "# Start Training\n",
    "TODO: make training script compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ./models directory\n",
    "Path(f\"models\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in folder A: 376\n",
      "Number of images in folder B: 318\n"
     ]
    }
   ],
   "source": [
    "# Get filenames\n",
    "train_A = load_data(img_dirA+\"/*.*\")\n",
    "train_B = load_data(img_dirB+\"/*.*\")\n",
    "\n",
    "global fns_all_trn_data\n",
    "fns_all_trn_data = train_A + train_B\n",
    "\n",
    "assert len(train_A), \"No image found in \" + str(img_dirA)\n",
    "assert len(train_B), \"No image found in \" + str(img_dirB)\n",
    "print (\"Number of images in folder A: \" + str(len(train_A)))\n",
    "print (\"Number of images in folder B: \" + str(len(train_B)))\n",
    "\n",
    "if use_bm_eyes:\n",
    "    assert len(glob.glob(img_dirA_bm_eyes+\"/*.*\")), \"No binary mask found in \" + str(img_dirA_bm_eyes)\n",
    "    assert len(glob.glob(img_dirB_bm_eyes+\"/*.*\")), \"No binary mask found in \" + str(img_dirB_bm_eyes)\n",
    "    assert len(glob.glob(img_dirA_bm_eyes+\"/*.*\")) == len(train_A), \\\n",
    "    \"Number of faceA images does not match number of their binary masks. Can be caused by any none image file in the folder.\"\n",
    "    assert len(glob.glob(img_dirB_bm_eyes+\"/*.*\")) == len(train_B), \\\n",
    "    \"Number of faceB images does not match number of their binary masks. Can be caused by any none image file in the folder.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_loss_config(loss_config):\n",
    "    for config, value in loss_config.items():\n",
    "        print(f\"{config} = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random binary masks of eyes\n",
    "train_batchA = create_minibatch(train_A, batchSize, img_dirA_bm_eyes)\n",
    "train_batchB = create_minibatch(train_B, batchSize, img_dirB_bm_eyes)\n",
    "_, _, tA, bmA = next(train_batchA)  \n",
    "_, _, tB, bmB = next(train_batchB)\n",
    "showG_eyes(tA, tB, bmA, bmB, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_session(save_path):\n",
    "    global model, vggface\n",
    "    model.save_weights(path=save_path)\n",
    "    del model\n",
    "    del vggface\n",
    "    K.clear_session()\n",
    "    model = FaceswapGANModel(**arch_config)\n",
    "    model.load_weights(path=save_path)\n",
    "    vggface = VGGFace(include_top=False, model='resnet50', input_shape=(224, 224, 3))\n",
    "    model.build_pl_model(vggface_model=vggface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "t0 = time.time()\n",
    "gen_iterations = 0\n",
    "errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
    "errGAs = {}\n",
    "errGBs = {}\n",
    "# Dictionaries are ordered in Python 3.6\n",
    "for k in ['ttl', 'adv', 'recon', 'edge', 'pl']:\n",
    "    errGAs[k] = 0\n",
    "    errGBs[k] = 0\n",
    "\n",
    "display_iters = 300\n",
    "backup_iters = 5000\n",
    "TOTAL_ITERS = 40000\n",
    "\n",
    "train_batchA = create_minibatch(train_A, batchSize, img_dirA_bm_eyes)\n",
    "train_batchB = create_minibatch(train_B, batchSize, img_dirB_bm_eyes)\n",
    "\n",
    "while gen_iterations <= TOTAL_ITERS:  \n",
    "    data_A = next(train_batchA) \n",
    "    data_B = next(train_batchB) \n",
    "    \n",
    "    # Loss function automation\n",
    "    if gen_iterations == (TOTAL_ITERS//5 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = False\n",
    "        loss_config['m_mask'] = 0.0\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    elif gen_iterations == (TOTAL_ITERS//5 + TOTAL_ITERS//10 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = True\n",
    "        loss_config['m_mask'] = 0.5\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Complete.\")\n",
    "    elif gen_iterations == (2*TOTAL_ITERS//5 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = True\n",
    "        loss_config['m_mask'] = 0.2\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    elif gen_iterations == (TOTAL_ITERS//2 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = True\n",
    "        loss_config['m_mask'] = 0.4\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    elif gen_iterations == (2*TOTAL_ITERS//3 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = False\n",
    "        loss_config['m_mask'] = 0.\n",
    "        loss_config['lr_factor'] = 0.3\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    elif gen_iterations == (8*TOTAL_ITERS//10 - display_iters//2):\n",
    "        clear_output()\n",
    "        model.decoder_A.load_weights(\"models/decoder_B.h5\") # swap decoders\n",
    "        model.decoder_B.load_weights(\"models/decoder_A.h5\") # swap decoders\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = True\n",
    "        loss_config['m_mask'] = 0.1\n",
    "        loss_config['lr_factor'] = 0.3\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    elif gen_iterations == (9*TOTAL_ITERS//10 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = False\n",
    "        loss_config['m_mask'] = 0.0\n",
    "        loss_config['lr_factor'] = 0.1\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    if gen_iterations == 5:\n",
    "        print (\"working.\")\n",
    "    \n",
    "    # Train dicriminators for one batch\n",
    "    errDA, errDB = model.train_one_batch_D(data_A=data_A, data_B=data_B)\n",
    "    errDA_sum +=errDA[0]\n",
    "    errDB_sum +=errDB[0]\n",
    "\n",
    "    # Train generators for one batch\n",
    "    errGA, errGB = model.train_one_batch_G(data_A=data_A, data_B=data_B)\n",
    "    errGA_sum += errGA[0]\n",
    "    errGB_sum += errGB[0]\n",
    "    for i, k in enumerate(['ttl', 'adv', 'recon', 'edge', 'pl']):\n",
    "        errGAs[k] += errGA[i]\n",
    "        errGBs[k] += errGB[i]\n",
    "    gen_iterations+=1\n",
    "    \n",
    "    # Visualization\n",
    "    if gen_iterations % display_iters == 0:\n",
    "        clear_output()\n",
    "            \n",
    "        # Display loss information\n",
    "        show_loss_config(loss_config)\n",
    "        print(\"----------\") \n",
    "        print('[iter %d] Loss_DA: %f Loss_DB: %f Loss_GA: %f Loss_GB: %f time: %f'\n",
    "        % (gen_iterations, errDA_sum/display_iters, errDB_sum/display_iters,\n",
    "           errGA_sum/display_iters, errGB_sum/display_iters, time.time()-t0))  \n",
    "        print(\"----------\") \n",
    "        print(\"Generator loss details:\")\n",
    "        print(f'[Adversarial loss]')  \n",
    "        print(f'GA: {errGAs[\"adv\"]/display_iters:.4f} GB: {errGBs[\"adv\"]/display_iters:.4f}')\n",
    "        print(f'[Reconstruction loss]')\n",
    "        print(f'GA: {errGAs[\"recon\"]/display_iters:.4f} GB: {errGBs[\"recon\"]/display_iters:.4f}')\n",
    "        print(f'[Edge loss]')\n",
    "        print(f'GA: {errGAs[\"edge\"]/display_iters:.4f} GB: {errGBs[\"edge\"]/display_iters:.4f}')\n",
    "        if loss_config['use_PL'] == True:\n",
    "            print(f'[Perceptual loss]')\n",
    "            try:\n",
    "                print(f'GA: {errGAs[\"pl\"][0]/display_iters:.4f} GB: {errGBs[\"pl\"][0]/display_iters:.4f}')\n",
    "            except:\n",
    "                print(f'GA: {errGAs[\"pl\"]/display_iters:.4f} GB: {errGBs[\"pl\"]/display_iters:.4f}')\n",
    "        \n",
    "        # Display images\n",
    "        print(\"----------\") \n",
    "        _, wA, tA, _ = next(train_batchA)  \n",
    "        _, wB, tB, _ = next(train_batchB)\n",
    "        print(\"Transformed (masked) results:\")\n",
    "        showG(tA, tB, model.path_A, model.path_B, batchSize)   \n",
    "        print(\"Masks:\")\n",
    "        showG_mask(tA, tB, model.path_mask_A, model.path_mask_B, batchSize)  \n",
    "        print(\"Reconstruction results:\")\n",
    "        showG(wA, wB, model.path_bgr_A, model.path_bgr_B, batchSize)           \n",
    "        errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
    "        for k in ['ttl', 'adv', 'recon', 'edge', 'pl']:\n",
    "            errGAs[k] = 0\n",
    "            errGBs[k] = 0\n",
    "        \n",
    "        # Save models\n",
    "        model.save_weights(path=models_dir)\n",
    "    \n",
    "    # Backup models\n",
    "    if gen_iterations % backup_iters == 0: \n",
    "        bkup_dir = f\"{models_dir}/backup_iter{gen_iterations}\"\n",
    "        Path(bkup_dir).mkdir(parents=True, exist_ok=True)\n",
    "        model.save_weights(path=bkup_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tf'></a>\n",
    "# Single Image Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_n_scale(src_img, tar_img):\n",
    "    \"\"\"\n",
    "    A color correction method\n",
    "    \"\"\"\n",
    "    mt = np.mean(tar_img, axis=(0,1))\n",
    "    st = np.std(tar_img, axis=(0,1))\n",
    "    ms = np.mean(src_img, axis=(0,1))\n",
    "    ss = np.std(src_img, axis=(0,1))    \n",
    "    if ss.any() <= 1e-7: return src_img    \n",
    "    result = st * (src_img.astype(np.float32) - ms) / (ss+1e-7) + mt\n",
    "    result = (255.0 / result.max() * result).astype(np.float32)\n",
    "    return result\n",
    "\n",
    "def transform_face(inp_img, direction=\"AtoB\", roi_coef=15, color_correction=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        inp_img: A RGB face image of any size.\n",
    "        direction:  A string that is either AtoB or BtoA\n",
    "        roi_coef: A coefficient that affects the cropped center area\n",
    "        color_correction: boolean, whether use color correction or not\n",
    "    Returns:\n",
    "        result_img: A RGB swapped face image after masking.\n",
    "        result_mask: The alpha mask which corresponds to the result_img.\n",
    "    \"\"\"\n",
    "    def get_feather_edges_mask(img, roi_coef):\n",
    "        img_size = img.shape\n",
    "        mask = np.zeros_like(img)\n",
    "        x_, y_ = img_size[0]//roi_coef, img_size[1]//roi_coef\n",
    "        mask[x_:-x_, y_:-y_,:]  = 255\n",
    "        mask = cv2.GaussianBlur(mask,(15,15),10).astype(np.float32) / 255\n",
    "        return mask        \n",
    "\n",
    "    if direction == \"AtoB\":\n",
    "        path_func = model.path_abgr_B\n",
    "    elif direction == \"BtoA\":\n",
    "        path_func = model.path_abgr_A\n",
    "    else:\n",
    "        raise ValueError(f\"direction should be either AtoB or BtoA, recieved {direction}.\")\n",
    "\n",
    "    # pre-process input image\n",
    "    img_bgr = cv2.cvtColor(inp_img, cv2.COLOR_RGB2BGR)\n",
    "    input_size = img_bgr.shape    \n",
    "    roi_x, roi_y = input_size[0]//roi_coef, input_size[1]//roi_coef\n",
    "    roi = img_bgr[roi_x:-roi_x, roi_y:-roi_y,:] # BGR, [0, 255]  \n",
    "    roi_size = roi.shape\n",
    "    ae_input = cv2.resize(roi, (RESOLUTION,RESOLUTION)) / 255. * 2 - 1 # BGR, [-1, 1]       \n",
    "\n",
    "    # post-process transformed roi image\n",
    "    ae_output = np.squeeze(np.array([path_func([[ae_input]])]))\n",
    "    ae_output_a = ae_output[:,:,0] * 255\n",
    "    ae_output_a = cv2.resize(ae_output_a, (roi_size[1],roi_size[0]))[...,np.newaxis]\n",
    "    ae_output_bgr = np.clip( (ae_output[:,:,1:] + 1) * 255 / 2, 0, 255)\n",
    "    ae_output_bgr = cv2.resize(ae_output_bgr, (roi_size[1],roi_size[0]))\n",
    "    ae_output_masked = (ae_output_a/255 * ae_output_bgr + (1 - ae_output_a/255) * roi).astype('uint8') # BGR, [0, 255]\n",
    "    if color_correction:\n",
    "        ae_output_masked = shift_n_scale(ae_output_masked, roi)\n",
    "\n",
    "    # merge transformed output back to input image\n",
    "    blend_mask = get_feather_edges_mask(roi, roi_coef)        \n",
    "    blended_img = blend_mask * ae_output_masked + (1 - blend_mask) * roi\n",
    "    result = img_bgr\n",
    "    result[roi_x:-roi_x, roi_y:-roi_y,:] = blended_img \n",
    "    result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB) \n",
    "    result_alpha = np.zeros_like(img_bgr)\n",
    "    result_alpha[roi_x:-roi_x, roi_y:-roi_y,:] = blend_mask * ae_output_a \n",
    "    return result, result_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = plt.imread(\"./TEST_IMAGE.jpg\")[...,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_img, result_mask = transform_face(input_img, direction=\"BtoA\", roi_coef=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(result_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(result_mask[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
